{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79551282a624bc61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T18:07:06.787818Z",
     "start_time": "2024-10-19T18:07:06.367468Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff83926ee4e455b",
   "metadata": {},
   "source": [
    "1. Download Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e729fedfef78a97a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T18:07:11.524078Z",
     "start_time": "2024-10-19T18:07:10.414723Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b328586-15fe-49ff-b764-7e45a0d81e46",
   "metadata": {},
   "source": [
    "2. Perform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d155e8-ecf1-4659-b774-516ac4214b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text: str) -> str:\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special character\n",
    "\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    # text = re.sub(r'[^a-z\\s\\.]', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = WhitespaceTokenizer().tokenize(text)\n",
    "\n",
    "    # Lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # stemmer = PorterStemmer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "    # text = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words and len(word) > 1])\n",
    "    \n",
    "    return text.replace('_', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e24d060-0621-4a87-8d30-f5d33e6190a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887201af-7264-488a-ba3d-3f0f1603f224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('chapter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f86685-dde3-48ce-b428-d937c62aead4",
   "metadata": {},
   "source": [
    "3. Find Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e53477-defb-42d8-8ea9-7688d75f3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_pattern = r\"chapter\\s+\\w+\"\n",
    "chapters = re.split(chapter_pattern, text, flags=re.IGNORECASE)\n",
    "chapters = chapters[13:]\n",
    "# chapters = list(map(lambda x: x.strip(), chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae861842-a87a-490c-a35e-c4c7ca81921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ae1c47-c96d-4644-9b45-c4576a329ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 500.89it/s]\n"
     ]
    }
   ],
   "source": [
    "chapters_dict = {}\n",
    "for chapter_num, chapter in tqdm(enumerate(chapters)):\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([chapter])\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    top_words = sorted(zip(feature_names, tfidf_matrix.toarray()[0]), key=lambda x: x[1], reverse=True)[:11]\n",
    "    chapters_dict[chapter_num + 1] = \", \".join([word for index, (word, score) in enumerate(top_words)\n",
    "                                                         if word != 'alice' and index < 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac0e09a5-5c6c-4d56-a17d-0a1a2cb10d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'wa, little, like, think, way, see, door, one, could, said',\n",
       " 2: 'wa, mouse, little, oh, said, dear, go, thing, foot, like',\n",
       " 3: 'said, mouse, wa, dodo, know, one, soon, bird, dry, long',\n",
       " 4: 'wa, little, one, rabbit, bill, said, get, heard, sure, thought',\n",
       " 5: 'said, wa, caterpillar, pigeon, serpent, little, well, know, minute, one',\n",
       " 6: 'said, wa, cat, like, duchess, little, baby, footman, mad, much',\n",
       " 7: 'said, hatter, dormouse, wa, march, hare, time, know, thing, well',\n",
       " 8: 'said, wa, queen, head, king, cat, three, hedgehog, like, one',\n",
       " 9: 'said, turtle, mock, wa, gryphon, duchess, queen, went, never, little',\n",
       " 10: 'said, gryphon, turtle, mock, would, dance, lobster, wa, soup, beautiful',\n",
       " 11: 'said, wa, king, hatter, court, dormouse, one, witness, queen, began',\n",
       " 12: 'said, king, would, wa, jury, little, queen, know, head, one'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d1b20-c464-452a-af49-fff97d2baf31",
   "metadata": {},
   "source": [
    "4. Find the Top 10 most used verbs in sentences with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c9fee0a-76f5-46eb-b833-4c8951833e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c134e7fe-72a0-4db7-a350-7387be7583e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddde7c9-ba20-45d2-884b-707d7bec3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(preprocessing, sentences))    \n",
    "sentences = list(map(lambda x: x.replace('.', ' '), sentences))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f2e8da-5ad1-470f-ba1e-e1ec73eb0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_sentences = [sentence for sentence in sentences if 'alice' in sentence.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d70c2ef-4e32-4011-84c9-8b88072b8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db9a495a-06be-46fb-b754-bdf8d50e4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in alice_sentences:\n",
    "    for word, tag in nltk.pos_tag(sentence.split()):\n",
    "        if tag in ['VB', 'VBP']:\n",
    "            verb_counts[word] += 1\n",
    "top_verbs = verb_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bf3ccdf-d9fe-4e16-af1b-7c99cc3ae297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say', 35),\n",
       " ('know', 32),\n",
       " ('go', 27),\n",
       " ('see', 23),\n",
       " ('think', 21),\n",
       " ('get', 17),\n",
       " ('make', 15),\n",
       " ('come', 14),\n",
       " ('take', 12),\n",
       " ('wa', 11)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567d928-8312-4be8-b555-858046f6c404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
